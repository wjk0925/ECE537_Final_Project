{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00cece2-eedb-46a3-b92a-0fd506db06fa",
   "metadata": {},
   "source": [
    "### Dataset (CS447 MT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1647e86-90f1-48c0-8308-211c5fb99821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, basename, dirname\n",
    "\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69a0c97-cb84-4821-8c79-95c2ea0e19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_dir = \"/u/junkaiwu/data/LJSpeech-1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1f4d76c-b56a-4144-87c0-4a4feebce80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cs447\n",
    "def unicode_to_ascii(s):\n",
    "    \"\"\"Normalizes latin chars with accent to their canonical decomposition\"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    '''\n",
    "    Preprocess the sentence to add the start, end tokens and make them lower-case\n",
    "    '''\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n",
    "    w = re.sub(r'[\" \"]+', ' ', w)\n",
    "\n",
    "    w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "075c2d76-bdc7-4869-b3a9-d64822185643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> printing , in the only sense with which we are at present concerned , differs from most if not from all the arts and crafts represented in the exhibition <end>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0053e17f-3558-4731-a45b-a13548b09fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_dict = {}\n",
    "\n",
    "with open(join(lj_dir, \"metadata.csv\"), \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        [filename, transcription, _] = line.strip(\"\\n\").split(\"|\")\n",
    "        transcription = preprocess_sentence(transcription)\n",
    "        lj_dict[filename] = transcription       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00743664-019d-4111-995e-ca3b29551251",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_datasets_dir = \"../datasets/LJSpeech/hubert100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "17fa97b3-c80d-4f70-9313-13cbf6751f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    w_f = open(join(txt_datasets_dir, split +\"2.txt\"), \"w\")\n",
    "    with open(join(txt_datasets_dir, split+\".txt\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            line_dict = eval(line.strip(\"\\n\"))\n",
    "            line_dict['transcription'] = lj_dict[basename(line_dict['audio']).strip('.wav')]\n",
    "            w_line = str(line_dict) + \"\\n\"\n",
    "            w_f.write(w_line)\n",
    "    w_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34e13041-97f6-4eb9-8a52-5be892a85237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'audio': 'data/LJSpeech-1.1/wavs_16khz/LJ001-0006.wav', 'hubert': '71 71 38 38 38 38 38 38 38 38 38 38 44 80 81 81 2 31 41 83 84 84 96 96 67 54 54 57 57 93 86 53 53 85 85 53 53 62 29 28 28 28 60 60 70 70 14 48 48 46 46 30 30 65 99 99 99 99 60 2 50 50 87 38 38 44 44 18 21 21 95 95 95 23 23 42 42 80 80 18 37 37 86 53 53 44 80 18 18 66 57 47 59 33 33 91 91 91 91 91 91 43 43 43 6 15 36 36 7 7 42 42 88 88 88 88 88 88 81 81 81 84 57 96 96 55 55 55 39 39 39 39 67 67 54 54 57 57 93 93 82 87 87 91 91 91 9 9 9 74 2 31 10 10 10 10 57 86 86 86 9 9 29 28 28 23 44 44 80 80 85 73 73 1 66 89 29 28 28 28 7 87 87 91 91 91 91 38 38 43 16 74 2 2 47 47 19 19 19 19 19 19 19 37 37 37 86 73 73 16 3 3 3 3 3 3 3 3 24 24 24 13 13 13 13 58 58 32 44 18 18 2 27 31 59 33 33 13 58 58 58 74 74 27 47 47 33 33 24 61 61 61 61 43 1 1 78 78 52 52 73 73 65 3 3 3 3 77 11 11 11 11 11 11 64 64 64 81 83 84 84 96 96 20 72 72 20', 'duration': 5.76}\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e9a99ee-8b10-41cd-a2c0-a5c0ea769e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b409435-4ef4-41d3-9aed-b4f0ca1dd1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
