{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6ed306-bb27-46d4-9183-aaf030c74fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert txt ljspeech files to tokens fairseq mt tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bd0c5c0-7d93-4f7c-adb9-115c3ee4df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7ed8bb7-b199-45fe-b9cf-a02c720644a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters\n",
    "vocab_size = 200\n",
    "endding = \"_noisy_v1\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986a7e0f-ced7-4b40-9530-3cbf7d0774ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nout_dir = f\"data/ljspeech_hubert{vocab_size}{endding}\"\\nassert not os.path.isdir(out_dir), print(\"Dir exists\")\\n\\nos.makedirs(out_dir)\\n\\nfor split in [\"train\", \"val\", \"test\"]:\\n    unit_wf_path = f\"{out_dir}/{split}.unit\"\\n    char_wf_path = f\"{out_dir}/{split}.char\"\\n    \\n    unit_wf = open(unit_wf_path, \"w\")\\n    char_wf = open(char_wf_path, \"w\")\\n\\n    with open(f\"../datasets/LJSpeech/hubert/{split}{vocab_size}{endding}.txt\", \"r\") as f:\\n        for line in f:\\n            utter_dict = eval(line.strip(\"\\n\"))\\n            unit = utter_dict[\"hubert\"]\\n            text = utter_dict[\"transcription\"]\\n            char = text.replace(\" \", \"|\")\\n            char = (\" \").join(char)\\n            unit_wf.write(unit+\"\\n\")\\n            char_wf.write(char+\"\\n\")\\n\\n    unit_wf.close()\\n    char_wf.close()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "out_dir = f\"data/ljspeech_hubert{vocab_size}{endding}\"\n",
    "assert not os.path.isdir(out_dir), print(\"Dir exists\")\n",
    "\n",
    "os.makedirs(out_dir)\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    unit_wf_path = f\"{out_dir}/{split}.unit\"\n",
    "    char_wf_path = f\"{out_dir}/{split}.char\"\n",
    "    \n",
    "    unit_wf = open(unit_wf_path, \"w\")\n",
    "    char_wf = open(char_wf_path, \"w\")\n",
    "\n",
    "    with open(f\"../datasets/LJSpeech/hubert/{split}{vocab_size}{endding}.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            utter_dict = eval(line.strip(\"\\n\"))\n",
    "            unit = utter_dict[\"hubert\"]\n",
    "            text = utter_dict[\"transcription\"]\n",
    "            char = text.replace(\" \", \"|\")\n",
    "            char = (\" \").join(char)\n",
    "            unit_wf.write(unit+\"\\n\")\n",
    "            char_wf.write(char+\"\\n\")\n",
    "\n",
    "    unit_wf.close()\n",
    "    char_wf.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a83180f-c6f2-49ed-b109-37da52275c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d369ab-dfcd-44bf-8335-ee216970d30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# check matches with original\\ngt_dict = {}\\nwith open(f\"/projects/bbmx/junkaiwu/LJSpeech-1.1/wavs_16khz/hubert_l6_v{vocab_size}.km\", \"r\") as f:\\n    for line in f:\\n        gt_dict[line.split(\"|\")[0]] = {}\\n        gt_dict[line.split(\"|\")[0]][\"unit\"] = line.split(\"|\")[1].strip(\"\\n\")\\n        \\nwith open(f\"/projects/bbmx/junkaiwu/LJSpeech-1.1/metadata.csv\", \"r\") as f:\\n    for line in f:\\n        gt_dict[line.split(\"|\")[0]][\"transcription_raw\"] = line.split(\"|\")[1].strip(\"\\n\")\\n        gt_dict[line.split(\"|\")[0]][\"transcription\"] = line.split(\"|\")[2].strip(\"\\n\")\\n        \\nfor split in [\"train\", \"val\", \"test\"]:\\n    with open(f\"../datasets/LJSpeech/hubert/{split}{vocab_size}.txt\", \"r\") as f:\\n        for line in f:\\n            utter_dict = eval(line.strip(\"\\n\"))\\n            assert utter_dict[\"hubert\"] == gt_dict[utter_dict[\"audio\"]][\"unit\"]\\n            assert utter_dict[\"transcription\"] == gt_dict[utter_dict[\"audio\"]][\"transcription\"]\\n            assert utter_dict[\"transcription_raw\"] == gt_dict[utter_dict[\"audio\"]][\"transcription_raw\"]\\n            \\nfor split in [\"train\", \"val\", \"test\"]:\\n    chars = []\\n    units = []\\n    with open(f\"data/ljspeech_hubert{vocab_size}/{split}.char\", \"r\") as f:\\n        for line in f:\\n            chars.append(line.strip(\"\\n\"))\\n\\n    with open(f\"data/ljspeech_hubert{vocab_size}/{split}.unit\", \"r\") as f:\\n        for line in f:\\n            units.append(line.strip(\"\\n\"))\\n    \\n    with open(f\"../datasets/LJSpeech/hubert/{split}{vocab_size}.txt\", \"r\") as f:\\n        for i, line in enumerate(f):\\n            utter_dict = eval(line.strip(\"\\n\"))\\n            assert utter_dict[\"hubert\"] == units[i]\\n            assert utter_dict[\"transcription\"] == chars[i].replace(\" \", \"\").replace(\"|\", \" \"), print(chars[i].replace(\" \", \"\").replace(\"|\", \"\")) \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# check matches with original\n",
    "gt_dict = {}\n",
    "with open(f\"/projects/bbmx/junkaiwu/LJSpeech-1.1/wavs_16khz/hubert_l6_v{vocab_size}.km\", \"r\") as f:\n",
    "    for line in f:\n",
    "        gt_dict[line.split(\"|\")[0]] = {}\n",
    "        gt_dict[line.split(\"|\")[0]][\"unit\"] = line.split(\"|\")[1].strip(\"\\n\")\n",
    "        \n",
    "with open(f\"/projects/bbmx/junkaiwu/LJSpeech-1.1/metadata.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        gt_dict[line.split(\"|\")[0]][\"transcription_raw\"] = line.split(\"|\")[1].strip(\"\\n\")\n",
    "        gt_dict[line.split(\"|\")[0]][\"transcription\"] = line.split(\"|\")[2].strip(\"\\n\")\n",
    "        \n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    with open(f\"../datasets/LJSpeech/hubert/{split}{vocab_size}.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            utter_dict = eval(line.strip(\"\\n\"))\n",
    "            assert utter_dict[\"hubert\"] == gt_dict[utter_dict[\"audio\"]][\"unit\"]\n",
    "            assert utter_dict[\"transcription\"] == gt_dict[utter_dict[\"audio\"]][\"transcription\"]\n",
    "            assert utter_dict[\"transcription_raw\"] == gt_dict[utter_dict[\"audio\"]][\"transcription_raw\"]\n",
    "            \n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    chars = []\n",
    "    units = []\n",
    "    with open(f\"data/ljspeech_hubert{vocab_size}/{split}.char\", \"r\") as f:\n",
    "        for line in f:\n",
    "            chars.append(line.strip(\"\\n\"))\n",
    "\n",
    "    with open(f\"data/ljspeech_hubert{vocab_size}/{split}.unit\", \"r\") as f:\n",
    "        for line in f:\n",
    "            units.append(line.strip(\"\\n\"))\n",
    "    \n",
    "    with open(f\"../datasets/LJSpeech/hubert/{split}{vocab_size}.txt\", \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            utter_dict = eval(line.strip(\"\\n\"))\n",
    "            assert utter_dict[\"hubert\"] == units[i]\n",
    "            assert utter_dict[\"transcription\"] == chars[i].replace(\" \", \"\").replace(\"|\", \" \"), print(chars[i].replace(\" \", \"\").replace(\"|\", \"\")) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "862cb62c-9529-4aed-ae97-a70efbba413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt=40\n",
    "beam=5\n",
    "\n",
    "preds = []\n",
    "names = []\n",
    "\n",
    "name2text = combine_json_files([\"../datasets/LJSpeech/ljspeech.json\"], None)\n",
    "\n",
    "text2name = {name2text[key][\"char\"]:key for key in name2text}\n",
    "\n",
    "w_f = open(f\"data-bin/ljspeech_hubert200/preds_{ckpt}_{beam}.txt\", \"w\")\n",
    "\n",
    "with open(f\"data-bin/ljspeech_hubert200/results_{ckpt}_{beam}.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if line[0] == \"S\":\n",
    "            text = line.split(\"\\t\")[1].strip(\"\\n\").replace(\" \", \"\").replace(\"|\", \" \")\n",
    "            name = text2name[text]\n",
    "            \n",
    "            names.append(name)\n",
    "\n",
    "        if line[0] == \"H\":\n",
    "            pred = line.split(\"\\t\")[2].strip(\"\\n\")\n",
    "            preds.append(pred)\n",
    "\n",
    "assert len(preds) == len(names) == 600\n",
    "\n",
    "for i in range(len(preds)): \n",
    "    utter_dict = {}\n",
    "    utter_dict[\"audio\"] = names[i]\n",
    "    utter_dict[\"hubert\"] = preds[i]\n",
    "    w_f.write(str(utter_dict)+\"\\n\")\n",
    "        \n",
    "w_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d35b1-4d31-4637-9140-78b8dd33442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fe076e6-5ca1-4fe4-b746-f63200eac3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'also their constant employment in labor appropriate to their condition.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99f4ede7-ae52-44d7-8452-e17b9b5a3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = {\"1\":\"a\", \"2\":\"b\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2aa78d0-0860-4b5b-93fd-7dbf14d18d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': '1', 'b': '2'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{aaa[key]:key for key in aaa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da57eb62-0af5-425b-b8fa-78bed9eab10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3700a9d1-4f5b-481b-82c3-2aa970ced909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_json_files(json_files, store_path, store=False):\n",
    "    # make sure they dont have same keys!\n",
    "    import json\n",
    "    from os.path import join, basename, dirname, isfile\n",
    "    dicts = []\n",
    "    for json_file in json_files:\n",
    "        with open(json_file) as f:\n",
    "            dicts.append(json.load(f))\n",
    "            \n",
    "    for i in range(1, len(dicts)):\n",
    "        dicts[0].update(dicts[i])\n",
    "        \n",
    "    if store and not isfile(store_path):\n",
    "        with open(store_path, \"w\") as f:\n",
    "            json.dump(dicts[0], f)\n",
    "        \n",
    "    return dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ef82a-ef5e-4be4-aa56-3762a4cd716d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
